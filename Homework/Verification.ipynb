{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение представлений и глубокое обучение, Домашнее задание 2\n",
    "## Верификация пар китайских иероглифов\n",
    "### https://inclass.kaggle.com/c/rdl-verification-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кашин Андрей, ШАД, Computer Science "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Make sure that caffe is on the python path:\n",
    "caffe_root = os.environ[\"CAFFE_ROOT\"]  # this file is expected to be in {caffe_root}/examples\n",
    "sys.path.insert(0, os.path.join(caffe_root, \"python\"))\n",
    "\n",
    "import caffe\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Фкнции для загрузки данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path, num_samples=None):\n",
    "    samples = []\n",
    "    samples_processed = 0\n",
    "    with open(path, \"r\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                image = np.load(f)\n",
    "                samples.append(image)\n",
    "                samples_processed += 1\n",
    "            except Exception, e:\n",
    "                break\n",
    "            if samples_processed % 10000 == 0:\n",
    "                print(\"Processed: {}\".format(samples_processed))\n",
    "            if num_samples and samples_processed >= num_samples:\n",
    "                break\n",
    "    return samples\n",
    "\n",
    "def load_labels(path, num_samples=None):\n",
    "    labels = pd.read_csv(path)\n",
    "    if num_samples:\n",
    "        return labels[\"Prediction\"].values[:num_samples]\n",
    "    else:\n",
    "        return labels[\"Prediction\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 10000\n",
      "Processed: 20000\n",
      "Processed: 30000\n",
      "Processed: 40000\n",
      "Processed: 50000\n",
      "Processed: 60000\n",
      "Processed: 70000\n",
      "Processed: 80000\n",
      "Processed: 90000\n",
      "Processed: 100000\n",
      "Processed: 110000\n",
      "Processed: 120000\n",
      "Processed: 130000\n",
      "Processed: 140000\n",
      "Processed: 150000\n",
      "Processed: 160000\n",
      "Processed: 170000\n",
      "Processed: 180000\n",
      "Processed: 190000\n",
      "Processed: 200000\n",
      "Processed: 10000\n",
      "Processed: 20000\n",
      "Data size: 200000\n",
      "Labels size: 200000\n"
     ]
    }
   ],
   "source": [
    "num_samples = None\n",
    "\n",
    "X = load_data(\"./data/Train/data.bin\", num_samples)\n",
    "X_test = load_data(\"./verification/data/Test/data.bin\", num_samples)\n",
    "y_orig = load_labels(\"./data/Train/train.csv\", num_samples)\n",
    "\n",
    "print(\"Data size: {}\".format(len(X)))\n",
    "print(\"Labels size: {}\".format(len(y_orig)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Масштабируем все картинки до одинакового размера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = 50\n",
    "w = 50\n",
    "\n",
    "Xr = np.array([scipy.misc.imresize(x, (h, w)).reshape((1, h, w)).astype(np.float64) for x in X])\n",
    "Xr_test = np.array([scipy.misc.imresize(x, (h, w)).reshape((1, h, w)).astype(np.float64) for x in X_test])\n",
    "y = y_orig[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализируем значения интенсивностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xr /= 256.0\n",
    "Xr_test /= 256.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = Xr.mean().mean().mean().mean()\n",
    "Xr -= mean\n",
    "Xr_test -= mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиваем данные на обучающую и тестовую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle data randomly\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "random.seed(17)\n",
    "    \n",
    "n_classes = 2000\n",
    "\n",
    "def shuffle(a, b):\n",
    "    indexes = range(len(a))\n",
    "    random.shuffle(indexes)\n",
    "    a[:] = a[indexes]\n",
    "    b[:] = b[indexes]\n",
    "\n",
    "print(\"Shuffle data randomly\")\n",
    "shuffle(Xr, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для того чтобы сделать сбалансированное разбиение (stratified split) нам необходимо выделить объекты каждого класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find class items\n"
     ]
    }
   ],
   "source": [
    "n_classes = 2000\n",
    "\n",
    "print(\"Find class items\")\n",
    "class_items = [[] for i in range(n_classes)]\n",
    "\n",
    "for i in range(len(y)):\n",
    "    if y[i] < n_classes:\n",
    "        class_items[y[i]].append(i)\n",
    "    \n",
    "class_items = np.array(class_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делаем разбиение в пропорциях 90% train / 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making train test split\n"
     ]
    }
   ],
   "source": [
    "print(\"Making train test split\")\n",
    "\n",
    "test_ratio = 0.1\n",
    "\n",
    "Xr_subset_train = []\n",
    "Xr_subset_test = []\n",
    "y_subset_train = []\n",
    "y_subset_test = []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    class_Xr = Xr[class_items[i]]\n",
    "    class_y = y[class_items[i]]\n",
    "    \n",
    "    test_size = test_ratio * len(class_Xr)\n",
    "    train_size = len(class_Xr) - test_size\n",
    "    \n",
    "    Xr_subset_train.append(class_Xr[:train_size])\n",
    "    y_subset_train.append(class_y[:train_size])\n",
    "    Xr_subset_test.append(class_Xr[train_size:])\n",
    "    y_subset_test.append(class_y[train_size:])\n",
    "    \n",
    "Xr_subset_train = np.array(Xr_subset_train)\n",
    "y_subset_train = np.array(y_subset_train)\n",
    "Xr_subset_test = np.array(Xr_subset_test)\n",
    "y_subset_test = np.array(y_subset_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Превращаем картинки в четырехмерный тензор для входа в caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xr_subset_train = Xr_subset_train.reshape(Xr_subset_train.shape[0] * Xr_subset_train.shape[1], 1, h, w)\n",
    "Xr_subset_test = Xr_subset_test.reshape(Xr_subset_test.shape[0] * Xr_subset_test.shape[1], 1, h, w)\n",
    "y_subset_train = y_subset_train.reshape(y_subset_train.shape[0] * y_subset_train.shape[1])\n",
    "y_subset_test = y_subset_test.reshape(y_subset_test.shape[0] * y_subset_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### На всякий случай еще раз перемешиваем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffle(Xr_subset_train, y_subset_train)\n",
    "shuffle(Xr_subset_test, y_subset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180000, 1, 50, 50)\n",
      "(20000, 1, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "print(Xr_subset_train.shape)\n",
    "print(Xr_subset_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция для генерации обучающих пар для сиамской сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate pairs data\n",
    "\n",
    "def generate_pairs(X, y, n_pairs=2):\n",
    "    np.random.seed(46)\n",
    "    class_items = [[] for i in range(n_classes)]\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] < n_classes:\n",
    "            class_items[y[i]].append(i)\n",
    "\n",
    "    class_items = np.array(class_items)\n",
    "\n",
    "    X_pairs = []\n",
    "\n",
    "    for first_class in range(n_classes):\n",
    "        for p in range(n_pairs):\n",
    "            same_class = np.random.rand() > 0.5\n",
    "            if same_class:\n",
    "                second_class = first_class\n",
    "            else:\n",
    "                second_class = np.random.randint(n_classes)\n",
    "\n",
    "            first_item = np.random.choice(class_items[first_class])\n",
    "            second_item = np.random.choice(class_items[second_class])\n",
    "\n",
    "            X_pairs.append((first_item, second_item, first_class == second_class))\n",
    "    \n",
    "    random.shuffle(X_pairs)\n",
    "    return X_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерируем обучающие и тестовые пары"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs_train = generate_pairs(Xr_subset_train, y_subset_train, n_pairs=40)\n",
    "pairs_test = generate_pairs(Xr_subset_test, y_subset_test, n_pairs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "print(len(pairs_train))\n",
    "print(len(pairs_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохраняем пары на диск\n",
    "\n",
    "#### Этот процесс можно повторить несколько раз с различными seed'ами, добавляя в обучающую выборку все новые пары"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write train data\n",
      "Write test data\n"
     ]
    }
   ],
   "source": [
    "# Write them to disk\n",
    "\n",
    "def create_image_pairs(X, y, pairs):\n",
    "    labels = np.array(map(lambda v: v[2], pairs))\n",
    "    data = np.array(map(lambda v: np.vstack([X[v[0]], X[v[1]]]), pairs))\n",
    "    return data, labels\n",
    "\n",
    "def write_pairs(X_train, y_train, pairs_train, X_test, y_test, pairs_test, folder, append=False):\n",
    "    import h5py\n",
    "\n",
    "    # Write out the data to HDF5 files in a temp directory.\n",
    "    dirname = os.path.abspath(folder)\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "\n",
    "    if append:\n",
    "        mode = 'a'\n",
    "    else:\n",
    "        mode = 'w'\n",
    "        \n",
    "    train_filename = os.path.join(dirname, 'train_' + str(h) + \"_4\" + '.h5')\n",
    "    test_filename = os.path.join(dirname, 'test_' + str(h) + \"_4\" + '.h5')\n",
    "    \n",
    "    print(\"Write train data\")\n",
    "    with h5py.File(train_filename, 'w') as f:\n",
    "        data, labels = create_image_pairs(X_train, y_train, pairs_train)\n",
    "        f.create_dataset('pair_data', data=data)\n",
    "        f.create_dataset('sim', data=labels.astype(np.float32))\n",
    "    with open(os.path.join(dirname, 'train.txt'), mode) as f:\n",
    "        f.write(train_filename + '\\n')\n",
    "\n",
    "    print(\"Write test data\")\n",
    "    with h5py.File(test_filename, 'w') as f:\n",
    "        data, labels = create_image_pairs(X_test, y_test, pairs_test)\n",
    "        f.create_dataset('pair_data', data=data)\n",
    "        f.create_dataset('sim', data=labels.astype(np.float32))\n",
    "    with open(os.path.join(dirname, 'test.txt'), mode) as f:\n",
    "        f.write(test_filename + '\\n')\n",
    "    \n",
    "write_pairs(Xr_subset_train, y_subset_train, pairs_train, Xr_subset_test, y_subset_test, pairs_test, \"verification/data\", append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Этап верификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем обученную модель с диска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_FILE = 'classification/network/lenet/lenet.prototxt'\n",
    "PRETRAINED = 'classification/network/lenet/data/train_iter_22000.caffemodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caffe.set_mode_gpu()\n",
    "net = caffe.Classifier(MODEL_FILE, PRETRAINED, image_dims=(50, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прогоняем тестовые картинки через сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 400\n",
      "Iteration: 800\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "batch_size = 25\n",
    "data_size = len(Xr_test)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in range((data_size + batch_size - 1) / batch_size):\n",
    "    if i % 400 == 0:\n",
    "        print(\"Iteration: {}\".format(i))\n",
    "    caffe_data =  np.array([x.reshape(1, h, w) for x in Xr_test[i * batch_size:min((i + 1) * batch_size, data_size)]])\n",
    "    net.forward(data=caffe_data)\n",
    "    prediction = net.blobs[\"prob\"].data\n",
    "    predictions.append(deepcopy(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разворачиваем батчи и выбираем класс с максимальной вероятностью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_predicted = np.array([x.argmax() for batch in predictions for x in batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Считываем пары, для которых необходимо построить предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs = open(\"verification/data/Test/test_pairs.csv\").readlines()\n",
    "pairs = pairs[1:]\n",
    "\n",
    "pairs = map(lambda s: s.strip().split(\",\"), pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Строим предсказание наивным образом просто сравнивая метки классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"verification/predictions/prediction_pairs.csv\", \"w\") as f:\n",
    "    f.write(\"Id,Prediction\\n\")\n",
    "        \n",
    "    for pair in pairs:\n",
    "        i = int(pair[0])\n",
    "        n1 = int(pair[1])\n",
    "        n2 = int(pair[2])\n",
    "        if y_predicted[n1] == y_predicted[n2]:\n",
    "            res = 1\n",
    "        else:\n",
    "            res = 0\n",
    "        f.write(\"{},{}\\n\".format(i, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузим сиамскую модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_FILE = 'verification/network/mnist_siamese/mnist_siamese.prototxt'\n",
    "PRETRAINED = 'verification/network/mnist_siamese/data/mnist_siamese_iter_38000.caffemodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caffe.set_mode_gpu()\n",
    "net = caffe.Classifier(MODEL_FILE, PRETRAINED, image_dims=(50, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Строим представление для каждой картинки из тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 400\n",
      "Iteration: 800\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "batch_size = 25\n",
    "data_size = len(Xr_test)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in range((data_size + batch_size - 1) / batch_size):\n",
    "    if i % 400 == 0:\n",
    "        print(\"Iteration: {}\".format(i))\n",
    "    caffe_data =  np.array([x.reshape(1, h, w) for x in Xr_test[i * batch_size:min((i + 1) * batch_size, data_size)]])\n",
    "    net.forward(data=caffe_data)\n",
    "    prediction = net.blobs[\"feat\"].data\n",
    "    predictions.append(deepcopy(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вытаскиваем представления из батчей в массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26325, 80)\n"
     ]
    }
   ],
   "source": [
    "feats = np.array([x for batch in predictions for x in batch])\n",
    "print(feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Строим предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros: 183802, Ones: 184748\n"
     ]
    }
   ],
   "source": [
    "zeros = 0\n",
    "ones = 0\n",
    "\n",
    "with open(\"verification/predictions/prediction_siamese.csv\", \"w\") as f:\n",
    "    f.write(\"Id,Prediction\\n\")\n",
    "        \n",
    "    for pair in pairs:\n",
    "        i = int(pair[0])\n",
    "        n1 = int(pair[1])\n",
    "        n2 = int(pair[2])\n",
    "        distance = np.linalg.norm(feats[n1] - feats[n2])\n",
    "        if distance < 0.68:\n",
    "            res = 1\n",
    "            ones += 1\n",
    "        else:\n",
    "            res = 0\n",
    "            zeros += 1\n",
    "        f.write(\"{},{}\\n\".format(i, res))\n",
    "print(\"Zeros: {}, Ones: {}\".format(zeros, ones))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Расстояние между двумя объектами в новом пространстве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79592383"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(feats[0] - feats[20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
