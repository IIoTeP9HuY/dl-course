{
 "metadata": {
  "name": "",
  "signature": "sha256:0efe9ce94da6ceadac5456392364ebd89bfc202b126435cb4f4fd330d03433e6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Seminar 5: Basic Artificial Neural Networks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal of this seminar is simple, yet an actual implementation may take some time :). We are going to write an Artificial Neural Network (almost) from scratch. The software design of the following scaffolding was heavily inspired by [Caffe](http://caffe.berkeleyvision.org/) which is a good thing as this package will be used extensively later in the course. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from time import time, sleep\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib import gridspec\n",
      "from IPython import display"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Perceptrons for digit classification "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, we are using [MNIST](http://yann.lecun.com/exdb/mnist/) as our dataset. First 60,000 samples are intended for training and validation (for simplicity, we'll use the whole chunk for training), and the rest 10,000 is a test set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from sklearn.datasets import fetch_mldata\n",
      "\n",
      "# Fetch MNIST dataset and create a local copy.\n",
      "if os.path.exists('mnist.npz'):\n",
      "    with np.load('mnist.npz', 'r') as data:\n",
      "        X = data['X']\n",
      "        y = data['y']\n",
      "else:\n",
      "    mnist = fetch_mldata(\"MNIST original\")\n",
      "    X, y = mnist.data / 255.0, mnist.target\n",
      "    np.savez('mnist.npz', X=X, y=y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Defining layers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each of the modern Deep Learning packages defines a set of **layers** which are used as building blocks for the complete Neural Network. In a nut shell, layer is some kind of transformation applied to the input data. It should be differentiable for the back-propagation to work. As it is usually done, the data is stored in so-called **blobs**. In our case blobs are just pairs of regular 2D matrices of size $ (N \\times C) $, where $ N $ is the number of samples in a batch, $ C $ is the number of channels in a sample. Here is a small example of a blob:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 100\n",
      "C = 512\n",
      "\n",
      "blob = {\n",
      "    'data': np.zeros((N, C)),\n",
      "    'diff': np.zeros((N, C)),\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The code above defines a blob for a batch of 100 samples with 512 channels per sample. The actual data is held in `blob['data']`, while `blob['diff']` is used as a storage for data gradients, i.e. $ \\frac{\\partial \\, \\mathtt{Objective}}{\\partial \\, \\mathtt{Data}} $.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code defines a common interface for every layer that we are going to use in our network. The **`bottom`** corresponds to the list of input blobs of the layer (some layers may receive multiple input blobs) and the **``top``** is a list (most commonly containing only one element) of output blobs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from abc import ABCMeta, abstractmethod\n",
      "\n",
      "class BaseLayer:\n",
      "    __metaclass__ = ABCMeta\n",
      "    \n",
      "    @abstractmethod\n",
      "    def reshape(self, bottom, top):\n",
      "        \"\"\"\n",
      "        Reshapes (resizes) top blobs accoring to the size of bottom blobs and internal\n",
      "        parameters of the layer. The idea here is to allocate all the blobs only once\n",
      "        avoiding repeated creation of arrays during the optimization process.\n",
      "\n",
      "        Keyword arguments:\n",
      "        bottom -- list of input blobs\n",
      "        top    -- list of output blobs\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def fprop(self, bottom, top):\n",
      "        \"\"\"\n",
      "        Applies layer transformation to the list of input blobs and writes the results\n",
      "        to the list of the output blobs.\n",
      "\n",
      "        Keyword arguments:\n",
      "        bottom -- list of input blobs\n",
      "        top    -- list of output blobs\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def bprop(self, top, bottom):\n",
      "        \"\"\"\n",
      "        Computes (d Objective) / (d Data) for all input blobs and stores the result for\n",
      "        the i-th blob to bottom[i]['diff'].\n",
      "\n",
      "        Keyword arguments:\n",
      "        bottom -- list of input blobs\n",
      "        top    -- list of output blobs\n",
      "        \"\"\"\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have defined the interface, it's time implement a bunch of actual layers. It's up to you to define more modules. Here's the complete example for the **Rectified Linear Unit** non-linearity (aka **ReLU**): "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class ReLULayer(BaseLayer):\n",
      "    def __init__(self):\n",
      "        pass\n",
      "    \n",
      "    def reshape(self, bottom, top):\n",
      "        top[0]['data'] = bottom[0]['data']\n",
      "        top[0]['diff'] = bottom[0]['diff']\n",
      "        self.mask = np.zeros(bottom[0]['data'].shape)\n",
      "    \n",
      "    def fprop(self, bottom, top):\n",
      "        np.maximum(bottom[0]['data'], 0.0, out=top[0]['data'])\n",
      "\n",
      "    def bprop(self, top, bottom):\n",
      "        self.mask[:] = bottom[0]['data'] > 0.0\n",
      "        np.multiply(top[0]['diff'], self.mask, out=bottom[0]['diff'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's your turn to implement the **Fully-connected** layer (``InnerProductLayer`` in the code) and the **Softmax + Multinomial Logistic Loss** combo (``SoftmaxLossLayer`` in the code):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The following 3 functors are needed for the weight initialization.\n",
      "\n",
      "class ConstantFiller:\n",
      "    '''Returns a matrix filled with the specified value.'''\n",
      "    def __init__(self, value):\n",
      "        self.value = value\n",
      "        \n",
      "    def __call__(self, shape):\n",
      "        return self.value * np.ones(shape)\n",
      "\n",
      "class GaussianFiller:\n",
      "    '''Returns a matrix filled with a gaussian noise ~ N(mu, sigma).'''\n",
      "    def __init__(self, mu, sigma):\n",
      "        self.mu = mu\n",
      "        self.sigma = sigma\n",
      "        \n",
      "    def __call__(self, shape):\n",
      "        return np.random.normal(self.mu, self.sigma, size=shape)\n",
      "    \n",
      "class XavierFiller:\n",
      "    '''\n",
      "    Returns a matrix filled uniform noise tailored to speed-up ANN learning.\n",
      "    \n",
      "    References:\n",
      "    * Glorot et al., 2010: \n",
      "    http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n",
      "    '''\n",
      "    def __init__(self):\n",
      "        pass\n",
      "        \n",
      "    def __call__(self, shape):\n",
      "        fan_in = shape[0]\n",
      "        fan_out = shape[1]\n",
      "        delta = np.sqrt(6.0 / (fan_in + fan_out))\n",
      "        return np.random.uniform(-delta, delta, size=shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class InnerProductLayer(BaseLayer):\n",
      "    \"\"\"\n",
      "    Fully-connected layer. Computes W * input + b.\n",
      "    \"\"\"\n",
      "    def __init__(self, channels, filters, **keywords):\n",
      "        self.filters = filters\n",
      "        self.channels = channels\n",
      "        \n",
      "        self.params = {\n",
      "            'W': {\n",
      "                'data': np.zeros((filters, channels)),\n",
      "                'diff': np.zeros((filters, channels))\n",
      "            },\n",
      "            'b': {\n",
      "                'data': np.zeros((filters, 1)),\n",
      "                'diff': np.zeros((filters, 1))\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        for k in self.params.keys():\n",
      "            self.params[k]['data'][:] = keywords[k](self.params[k]['data'].shape)\n",
      "            \n",
      "    def reshape(self, bottom, top):\n",
      "        # Your code goes here. ################################################\n",
      "    \n",
      "    def fprop(self, bottom, top):\n",
      "        # Your code goes here. ################################################\n",
      "\n",
      "    def bprop(self, top, bottom):\n",
      "        # Your code goes here. ################################################   \n",
      "        \n",
      "class SoftmaxLossLayer(BaseLayer):\n",
      "    \"\"\"\n",
      "    Computes the multinomial logistic loss of the softmax of its inputs. It\u2019s \n",
      "    conceptually identical to a softmax layer followed by a multinomial logistic \n",
      "    loss layer, but provides a more numerically stable gradient.\n",
      "    \n",
      "    References:\n",
      "    * Softmax:\n",
      "    http://en.wikipedia.org/wiki/Softmax_function\n",
      "    * Multinomial logistic loss:\n",
      "    http://en.wikipedia.org/wiki/Multinomial_logistic_regression\n",
      "    http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1SoftmaxWithLossLayer.html\n",
      "    http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1MultinomialLogisticLossLayer.html#details\n",
      "    \"\"\"\n",
      "    def __init__(self):\n",
      "        pass\n",
      "    \n",
      "    def reshape(self, bottom, top):\n",
      "        # Your code goes here. ################################################ \n",
      "    \n",
      "    def fprop(self, bottom, top):\n",
      "        # Your code goes here. ################################################ \n",
      "\n",
      "    def bprop(self, top, bottom):\n",
      "        # Your code goes here. ################################################ "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Defining a complete model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are going to assemble freshly coded layers into a complete model by means of additional class ``MLP`` (i.e. **Multi-layer perceptron**). Before we start, let's briefly look at how one would use it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# DO NOT EVALUATE THIS CELL!\n",
      "\n",
      "# First we create a sequence of layers. The following model will receive\n",
      "# 100D inputs, transform them in 200D outputs, apply non-linearity\n",
      "# and linear 10D mapping. The resulting vectors are normalized using \n",
      "# Softmax and interpreted as class probabilities. Those probabilities \n",
      "# are fed into the multinomial logistic loss function (along with \n",
      "# ground-truth labels) in order to compute the objective.\n",
      "input_dim = 100\n",
      "hidden_dim = 200\n",
      "n_classes = 10\n",
      "\n",
      "layers = [\n",
      "    InnerProductLayer(input_dim, hidden_dim, W=XavierFiller(), b=ConstantFiller(0.0)),\n",
      "    ReLULayer(),\n",
      "    InnerProductLayer(hidden_dim, n_classes, W=XavierFiller(), b=ConstantFiller(0.0)),\n",
      "    SoftmaxLossLayer()\n",
      "]\n",
      "\n",
      "# The sequence is used to create a model instance. \n",
      "model = MLP(desc)\n",
      "\n",
      "# Let's create some dummy input batch of 10 samples.\n",
      "n_samples = 10\n",
      "input_data = np.random.normal(0, 0.1, size=(n_samples, input_dim))\n",
      "\n",
      "# As we are solving the classification task we need a set of labels.\n",
      "labels = numpy.random.choice(np.arange(10), size=n_samples)\n",
      "\n",
      "# Now we set models inputs. This should indirectly invoke internal blobs \n",
      "# creation via reshape methods.\n",
      "model.set_input(input_data, labels)\n",
      "    \n",
      "# Here is where the magic happens. Forward propagation and \n",
      "# backward propagation.\n",
      "model.fprop()\n",
      "model.bprop()\n",
      "    \n",
      "# At this point we can get the value of the objective ...\n",
      "objective = model.get_loss()\n",
      "# ... and inspect the model parameters (i.e. list of all tunable parameters \n",
      "# in the network).\n",
      "for param in model.params:\n",
      "    print('Max weight in the blob:', param['data'].max())\n",
      "    print('Max weight gradient in the blob:', param['diff'].max())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok, let's proceed to the implementation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MLP:\n",
      "    \"\"\"\n",
      "    Multi-layer perceptron class.\n",
      "    \"\"\"\n",
      "    def __init__(self, desc):\n",
      "        self.layers = desc\n",
      "        self.reshaped = False\n",
      "        \n",
      "        # Here we aggregate all tunable parameters into a single list.\n",
      "        self.params = []\n",
      "        for l in self.layers:\n",
      "            try:\n",
      "                for v in l.params.values():\n",
      "                    self.params.append(v)\n",
      "            except Exception, e:\n",
      "                pass\n",
      "    \n",
      "    def reshape(self, input_shape):\n",
      "        \"\"\"\n",
      "        Invokes reshape methods of all layers.\n",
      "        \"\"\"\n",
      "        batch_size = input_shape[0]\n",
      "        self.data = {\n",
      "            'data': np.zeros(input_shape), \n",
      "            'diff': np.zeros(input_shape)\n",
      "        }\n",
      "        self.labels = {\n",
      "            'data': np.zeros((batch_size, 1), dtype=np.int32)\n",
      "        }\n",
      "        \n",
      "        self.blobs = [] # Holds all internal blobs created by reshape methods.\n",
      "        \n",
      "        # Your code goes here. ################################################ \n",
      "            \n",
      "        self.reshaped = True\n",
      "    \n",
      "    def set_input(self, data, labels):\n",
      "        if not self.reshaped:\n",
      "            self.reshape(data.shape)\n",
      "            \n",
      "        # Populate self.data and self.labels.\n",
      "        # Your code goes here. ################################################ \n",
      "        \n",
      "    def fprop(self):\n",
      "        \"\"\"\n",
      "        Conducts forward-propagation through the network.\n",
      "        \n",
      "        (i.e. fills self.blobs[:]['data'])\n",
      "        \"\"\"\n",
      "        # Your code goes here. ################################################ \n",
      "        \n",
      "        # NOTE: Keep in mind that the last layer should receive ground-truth \n",
      "        #       labels as well as blob from the lower layer.\n",
      "        \n",
      "    def bprop(self):\n",
      "        \"\"\"\n",
      "        Conducts backward-propagation through the network.\n",
      "        \n",
      "        (i.e. fills self.blobs[:]['diff'])\n",
      "        \"\"\"\n",
      "        # Your code goes here. ################################################ \n",
      "        \n",
      "    def get_loss(self):\n",
      "        \"\"\" Return the value of the objective function \"\"\"\n",
      "        return self.blobs[-1]['data'].mean()\n",
      "    \n",
      "    def test(self, data, labels):\n",
      "        \"\"\"\n",
      "        Helper function for evaluating the performance of the network on a test\n",
      "        set (which can be larger than the batch size).\n",
      "        \n",
      "        Returns accuracy.\n",
      "        \"\"\"\n",
      "        batch_size = self.data['data'].shape[0]\n",
      "        preds = []\n",
      "        for start in xrange(0, data.shape[0], batch_size):\n",
      "            self.set_input(data[start : start + batch_size, :], labels[start : start + batch_size])\n",
      "            self.fprop()\n",
      "            preds += [self.layers[-1].probs.argmax(axis=1)]\n",
      "        preds = np.hstack(preds)\n",
      "        return np.mean(preds == labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's sometimes useful to look at the network weights as that may give some insights of what the ANN has learned during training. The following code has been shamelessly copy-and-pasted from the original ``Caffe`` tutorial. We will be using that to visualize the state of the network. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# take an array of shape (n, height, width) or (n, height, width, channels)\n",
      "# and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\n",
      "def vis_square(data, padsize=1, padval=0):\n",
      "    data -= data.min()\n",
      "    data /= data.max()\n",
      "    \n",
      "    # force the number of filters to be square\n",
      "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
      "    padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n",
      "    data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n",
      "    \n",
      "    # tile the filters into an image\n",
      "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
      "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
      "    \n",
      "    plt.imshow(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Network training"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point we have implemented everything we need in order to finally train our ANN. We will be using **Stochastic Gradient Descent** (SGD) with momentum as the optimization algorithm. That should be familiar to you from the previous lecture. Compare single-layer linear perceptron and true multi-layer perceptron (say, with **500** hidden units and ReLU non-linearity)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's define the parameters of the optimization procedure.\n",
      "# Your code goes here. ######################################################## \n",
      "# n_epochs     = # This many times we will go through the whole training set\n",
      "# batch_size   = # This many samples are packed into a single batch\n",
      "\n",
      "# lr           = # Learning rate.\n",
      "# momentum     = # Momentum.\n",
      "# weight_decay = # L2-regularization coefficient.\n",
      "\n",
      "# test_iter    = # Perform testing of the model every this many iterations.\n",
      "\n",
      "n_samples = 60000\n",
      "n_batches_per_epoch = n_samples / batch_size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define a sequence of layers.\n",
      "# Your code goes here. ######################################################## \n",
      "# desc = []\n",
      "\n",
      "model = MLP(desc)\n",
      " \n",
      "training_loss = {\n",
      "    'ts': [],\n",
      "    'values': []\n",
      "}\n",
      "test_accuracy = {\n",
      "    'ts': [],\n",
      "    'values': []\n",
      "}\n",
      "\n",
      "indices = np.arange(n_samples)\n",
      "for t in xrange(n_epochs * n_batches_per_epoch):\n",
      "    i = t % n_batches_per_epoch\n",
      "    \n",
      "    # Shuffle dataset after each epoch.\n",
      "    if i == 0:\n",
      "        np.random.shuffle(indices)\n",
      "        \n",
      "    # Perform testing (using model.test)\n",
      "    if t % test_iter == 0 and t != 0:\n",
      "        # Your code goes here. ################################################ \n",
      "    \n",
      "    start = i * batch_size\n",
      "    end = (i + 1) * batch_size\n",
      "    batch_indices = indices[start : end]\n",
      "    \n",
      "    batch = (X[batch_indices, :], np.int32(y[batch_indices]))\n",
      "    \n",
      "    # Your code goes here. ####################################################\n",
      "    \n",
      "    # Visualize training process.\n",
      "    display.clear_output(wait=True)\n",
      "    plt.figure(figsize=(18, 6))\n",
      "    \n",
      "    gs = gridspec.GridSpec(1, 3, width_ratios=[1, 2, 2]) \n",
      "    \n",
      "    ax = plt.subplot(gs[0])\n",
      "    ax.set_title(\"Weights\")\n",
      "    vis_square(model.layers[0].params['W']['data'].reshape((-1, 28, 28))[: 25, :, :])\n",
      "\n",
      "    ax = plt.subplot(gs[1])\n",
      "    ax.set_title(\"Training loss\")\n",
      "    plt.plot(training_loss['ts'], training_loss['values'], 'b')\n",
      "    \n",
      "    ax = plt.subplot(gs[2])\n",
      "    ax.set_title(\"Test accuracy\")\n",
      "    plt.plot(test_accuracy['ts']), test_accuracy['values'], 'r')\n",
      "    \n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you are experiencing problems with convergence, try applying a schedule, e.g. something like `inv`-policy from `Caffe`:\n",
      "$$\n",
      "    \\mathtt{base\\_lr} \\cdot (1 + \\gamma \\cdot \\mathtt{t})^{-p} \\, .\n",
      "$$\n",
      "See `Caffe` [examples](https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_solver.prototxt) for typical values of $ \\gamma $ and $ p $."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "What has it learned?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a learned classification model and a class of interest one may numerically generate an image, which is representative of the class in terms of the ANN class scoring model.\n",
      "\n",
      "More formally, let $ S_c(I) $ be the score of the class $ c $, computed by the classification layer of the network for an image $ I $. We would like to find an $ \\mathcal{L}_2 $-regularized image, such that the score $ S_c $ is high:\n",
      "$$\n",
      "    \\arg\\max_{I} S_c(I) - \\lambda \\| I \\|_2^2 \\, ,\n",
      "$$\n",
      "where $ \\lambda $ is the regularisation parameter.\n",
      "\n",
      "For additional details see the paper by [Simonyan et al., 2010](http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/simonyan14deep.pdf)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Your task** is to get those reconstructed images for all 10 digits using previously trained multi-layer perceptron. This can be done by slightly modifying the optimization procedure implemented above. The following functions may come handy in that:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def dump_weights(filename, model):\n",
      "    \"\"\" Dumps all tunable parameters of the model into a file. \"\"\"\n",
      "    with open(filename, 'wb') as f:\n",
      "        for param in model.params:\n",
      "            for v in param.values():\n",
      "                np.save(f, v)\n",
      "                \n",
      "def load_weights(filename, model):\n",
      "    \"\"\" \n",
      "    Loads all tunable parameters from file into the specified model. \n",
      "    \n",
      "    NOTE: The model doesn't have to be of the same architecture as the model\n",
      "          that was used to dump the parameters. The only requirement is\n",
      "          the order of the tunable parameters.\n",
      "    \"\"\"\n",
      "    with open(filename, 'rb') as f:\n",
      "        for param in model.params:\n",
      "            for v in param.values():\n",
      "                v[:] = np.load(f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Few tips:\n",
      "* You may want to define an additional loss function corresponding to value of the neuron.\n",
      "* As it pointed out in the original paper it better to optimize the output of the last linear layer rather than the output of Softmax.\n",
      "* Try starting with a zero image.\n",
      "\n",
      "Give few comments on obtained reconstructions."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}